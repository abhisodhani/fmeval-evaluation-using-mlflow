{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model evaluation metrics tracked with MLflow\n",
    "\n",
    "In this notebook we'll demonstrate how to download the fmeval metrics tracked using MLflow to create a visual comparison in the form of [radar or spider charts](https://en.wikipedia.org/wiki/Radar_chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from fmeval_mlflow import get_metrics_from_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the environmental variables `MLFLOW_TRACKING_URI` and `MLFLOW_TRACKING_USERNAME` from the `.env` file created in [00-Setup](./00-Setup.ipynb).\n",
    "Alternatively you can set the tracking URL using the `MLflow` SDK method:\n",
    "\n",
    "``` python\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We organize the metrics into two major categories: those for which a larger value is better, and those where a smaller value is better. These two classes will be plotted on separated charts, making the interpretation of the comparison easier and more immediate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_better = [\"factual_knowledge\", \"summarization_accuracy\"]\n",
    "smaller_better = [\"toxicity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting function\n",
    "This plotting function will make it easier to create consistet radar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_trace(values, categories, name: str):\n",
    "    mask = ~np.isnan(values)\n",
    "    values = values[mask]\n",
    "    categories = categories[mask]\n",
    "    return go.Scatterpolar(r=values, theta=categories, fill=\"toself\", name=name)\n",
    "\n",
    "\n",
    "def create_spider_fig(\n",
    "    df,\n",
    "    title: str | None = None,\n",
    "    fig: go.Figure | None = None,\n",
    "    aggregation: str = \"run_id\",\n",
    "):\n",
    "    if fig is None:\n",
    "        fig = go.Figure()\n",
    "    traces = df.groupby([aggregation])[[\"model_id\", \"metric\", \"value\"]].apply(\n",
    "        lambda x: create_trace(\n",
    "            x[\"value\"].values,\n",
    "            x[\"metric\"].values,\n",
    "            x[\"model_id\"].iloc[0],\n",
    "        )\n",
    "    )\n",
    "    for trace in traces:\n",
    "        fig.add_trace(trace)\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                # range=[0, max(values) + max(values) * 0.1]\n",
    "            )\n",
    "        ),\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-simple-runs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving metrics\n",
    "\n",
    "The retrieval of the metrics from the MLflow experiemnt is encapsultated in the uttlity function `get_metrics_from_experiment()`. You can check the details of the code in [uttls.py](uttls.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_metrics_from_experiment(experiment_name)\n",
    "metrics.pivot_table(\n",
    "    index=[\"evaluation\", \"metric\"], columns=[\"model_id\"], values=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_spider_fig(\n",
    "    metrics[metrics[\"evaluation\"].isin(larger_better)], aggregation=\"run_id\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_spider_fig(\n",
    "    metrics[metrics[\"evaluation\"].isin(smaller_better)], aggregation=\"run_id\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare nested runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-nested-runs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving metrics\n",
    "\n",
    "The retrieval of the metrics from the MLflow experiemnt is encapsultated in the uttlity function `get_metrics_from_experiment()`. You can check the details of the code in [uttls.py](uttls.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = get_metrics_from_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pivot_table(\n",
    "    index=[\"evaluation\", \"metric\"], columns=[\"model_id\"], values=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_spider_fig(\n",
    "    metrics[metrics[\"evaluation\"].isin(larger_better)],\n",
    "    aggregation=\"tags.mlflow.parentRunId\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_spider_fig(metrics[metrics[\"evaluation\"].isin(smaller_better)])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
