{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate an Amazon Bedrock model with FMeval and track with MLflow\n",
    "\n",
    "***\n",
    "Developed and tested on Jupyterlab App on Amazon SageMaker Studio, SageMaker Distribution 2.1.0, instance `ml.m5.2xlarge`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.eval_algorithms.factual_knowledge import (\n",
    "    FactualKnowledge,\n",
    "    FactualKnowledgeConfig,\n",
    ")\n",
    "from fmeval.eval_algorithms.summarization_accuracy import SummarizationAccuracy\n",
    "from fmeval.eval_algorithms.toxicity import Toxicity, ToxicityConfig\n",
    "from fmeval.model_runners.bedrock_model_runner import BedrockModelRunner\n",
    "\n",
    "from utils import EvaluationSet, run_evaluation_sets, run_evaluation_sets_nested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the environmental variables `MLFLOW_TRACKING_URI` and `MLFLOW_TRACKING_USERNAME` from the `.env` file created in [00-Setup](./00-Setup.ipynb).\n",
    "Alternatively you can set the tracking URL using the `mlflow` sdk method:\n",
    "\n",
    "``` python\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Runner Setup\n",
    "\n",
    "The model runner we create below will be used to perform inference on every sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"INSERT-BEDROCK-MODEL-ID-HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the model content template. We can find this information from the Amazon Bedrock console in the `API request` sample section, and look at value of the `body`. As an example, here is the content template for Claude 3 Heiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jmespath = \"content[0].text\"\n",
    "content_template = \"\"\"{\n",
    "  \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "  \"max_tokens\": 512,\n",
    "  \"temperature\": 0.5,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": $prompt\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\"\"\"\n",
    "\n",
    "model_runner = BedrockModelRunner(\n",
    "    model_id=model_id,\n",
    "    output=output_jmespath,\n",
    "    content_template=content_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We first check that the dataset file to be used by the evaluation is present, and then create a `DataConfig` object for each dataset. Each dataset has been prepared to evaluate one of the three categories, i.e., `Summarization`, `Factual Knowledge`, and `Toxicity`. More categories can be defined too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"datasets\")\n",
    "\n",
    "dataset_uri_summarization = dataset_path / \"gigaword_sample.jsonl\"\n",
    "if not dataset_uri_summarization.is_file():\n",
    "    print(\"ERROR - please make sure the file, gigaword_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_summarization = DataConfig(\n",
    "    dataset_name=\"gigaword_sample\",\n",
    "    dataset_uri=dataset_uri_summarization.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"document\",\n",
    "    target_output_location=\"summary\",\n",
    ")\n",
    "\n",
    "dataset_uri_factual_knowledge = dataset_path / \"trex_sample.jsonl\"\n",
    "if not dataset_uri_factual_knowledge.is_file():\n",
    "    print(\"ERROR - please make sure the file, trex_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_factual_knowledge = DataConfig(\n",
    "    dataset_name=\"trex_sample\",\n",
    "    dataset_uri=dataset_uri_factual_knowledge.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answers\",\n",
    ")\n",
    "\n",
    "dataset_uri_toxicity = dataset_path / \"real_toxicity_sample.jsonl\"\n",
    "if not dataset_uri_toxicity.is_file():\n",
    "    print(\"ERROR - please make sure the file, real_toxicity_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_toxicity = DataConfig(\n",
    "    dataset_name=\"toxicity_sample\",\n",
    "    dataset_uri=dataset_uri_toxicity.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation sets\n",
    "We can now create an evaluation set for each algorithm we want to use in our test.\n",
    "\n",
    "More information about metrics and evaluation sets can be found in the paper [\"Evaluating Large Language Models with <i>fmeval</i>\"](https://arxiv.org/pdf/2407.12872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_accuracy = SummarizationAccuracy()\n",
    "factual_knowledge = FactualKnowledge(\n",
    "    FactualKnowledgeConfig(target_output_delimiter=\"<OR>\")\n",
    ")\n",
    "toxicity = Toxicity(ToxicityConfig(\"detoxify\"))\n",
    "\n",
    "evaluation_set_summarization = EvaluationSet(\n",
    "    data_config_summarization,\n",
    "    summarization_accuracy,\n",
    "    \"Summarize the following text in one sentence: $model_input\",\n",
    ")\n",
    "\n",
    "evaluation_set_factual = EvaluationSet(\n",
    "    data_config_factual_knowledge,\n",
    "    factual_knowledge,\n",
    "    \"$model_input\",\n",
    ")\n",
    "\n",
    "evaluation_set_toxicity = EvaluationSet(\n",
    "    data_config_toxicity,\n",
    "    toxicity,\n",
    "    \"Complete the following sentence: $model_input\",\n",
    ")\n",
    "\n",
    "\n",
    "evaluation_list = [\n",
    "    evaluation_set_summarization,\n",
    "    evaluation_set_factual,\n",
    "    evaluation_set_toxicity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluations\n",
    "We setup the MLflow experiment used to track the evaluations.\n",
    "We will then create a new run for each model, and run all the evaluation for that model within that run, so that the metrics will all appear together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `model_id` as run name to make it easier to identify this run as part of the larger experiment, and run the evaluation using the `run_evaluation_sets()` defined in [utils.py](utils.py#20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-simple-runs\"\n",
    "experiment = mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_evaluation_sets(model_runner, evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested runs\n",
    "An alternative approach to organize the runs is to create nested runs for the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-nested-runs\"\n",
    "experiment = mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name, nested=True) as run:\n",
    "    run_evaluation_sets_nested(model_runner, evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "The evaluation is completed, and the results are recorded in the MLflow tracking server.\n",
    "\n",
    "To continue with the evaluation, you can move to the [compare_models.ipynb](./compare_models.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
