{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a SageMaker JumpStart model with FMeval and track with MLflow\n",
    "\n",
    "***\n",
    "Developed and tested on Jupyterlab App on Amazon SageMaker Studio, SageMaker Distribution 2.1.0, instance `ml.m5.2xlarge`\n",
    "***\n",
    "\n",
    "This notebook shows you how to use FMeval to evaluate a LLM deployed via SageMaker Jumpstart and track the evaluations as metrics with MLflow tracking server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.eval_algorithms.factual_knowledge import (\n",
    "    FactualKnowledge,\n",
    "    FactualKnowledgeConfig,\n",
    ")\n",
    "from fmeval.eval_algorithms.summarization_accuracy import SummarizationAccuracy\n",
    "from fmeval.eval_algorithms.toxicity import Toxicity, ToxicityConfig\n",
    "from fmeval.model_runners.sm_jumpstart_model_runner import JumpStartModelRunner\n",
    "from utils import EvaluationSet, run_evaluation_sets, run_evaluation_sets_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the environmental variables `MLFLOW_TRACKING_URI` and `MLFLOW_TRACKING_USERNAME` from the `.env` file created in [00-Setup](./00-Setup.ipynb).\n",
    "Alternatively you can set the tracking URL using the `mlflow` sdk method:\n",
    "\n",
    "``` python\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the SageMaker Jumpstart endpoint you want to test. You need the corresponding `model_id` in SageMaker Jumpstart. It can be found when navigating in SageMaker Studio to the JumpStart section and looking at the model details or the sample notebook associated with the deployment section.\n",
    "\n",
    "![jumpstart-model-id](../img/find-jumpstart-model-id.png)\n",
    "\n",
    "Alternatively, if you have an existing SageMaker Jumpstart endpoint, you can replace the cell below by setting only the `endpoint_name` variable\n",
    "\n",
    "```python\n",
    "endpoint_name = \"jumpstart-existing-endpoint-name\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_id = \"<JUMPSTARD-MODEL-ID>\"  # e.g., \"huggingface-llm-falcon2-11b\"\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "accept_eula = False  # <-- some Jumpstart models requires explicitly accepting a EULA\n",
    "predictor = model.deploy(accept_eula=accept_eula)\n",
    "endpoint_name = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<YOUR-EXISTING-JUMPSTART-ENDPOINT-NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Runner Setup\n",
    "\n",
    "The model runner we create below will be used to perform inference on every sample in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.jumpstart.session_utils import get_model_info_from_endpoint\n",
    "from sagemaker.predictor import retrieve_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract information about the model. One particularly important information is the `inputs` format, which tells us the prompt signature for the model we have deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version, _, _, _ = get_model_info_from_endpoint(\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)\n",
    "predictor = retrieve_default(endpoint_name=endpoint_name)\n",
    "sample_payload = model.retrieve_example_payload().body\n",
    "print(json.dumps(sample_payload, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(predictor.predict(sample_payload), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For JumpStart model, `FMeval`  gets payload and output formats from the description of the models, this make it easier to setup the runners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runner = JumpStartModelRunner(\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test our model runner. You should build the prompt according to the expected input signature of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runner.predict(prompt=\"What's the tallest building in the world?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We first check that the dataset file to be used by the evaluation is present, and then create a `DataConfig` object for each dataset. Each dataset has been prepared to evaluate one of the three categories, i.e., `Summarization`, `Factual Knowledge`, and `Toxicity`. More categories can be defined too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"datasets\")\n",
    "\n",
    "dataset_uri_summarization = dataset_path / \"gigaword_sample.jsonl\"\n",
    "if not dataset_uri_summarization.is_file():\n",
    "    print(\"ERROR - please make sure the file, gigaword_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_summarization = DataConfig(\n",
    "    dataset_name=\"gigaword_sample\",\n",
    "    dataset_uri=dataset_uri_summarization.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"document\",\n",
    "    target_output_location=\"summary\",\n",
    ")\n",
    "\n",
    "dataset_uri_factual_knowledge = dataset_path / \"trex_sample.jsonl\"\n",
    "if not dataset_uri_factual_knowledge.is_file():\n",
    "    print(\"ERROR - please make sure the file, trex_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_factual_knowledge = DataConfig(\n",
    "    dataset_name=\"trex_sample\",\n",
    "    dataset_uri=dataset_uri_factual_knowledge.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answers\",\n",
    ")\n",
    "\n",
    "dataset_uri_toxicity = dataset_path / \"real_toxicity_sample.jsonl\"\n",
    "if not dataset_uri_toxicity.is_file():\n",
    "    print(\"ERROR - please make sure the file, real_toxicity_sample.jsonl, exists.\")\n",
    "\n",
    "data_config_toxicity = DataConfig(\n",
    "    dataset_name=\"toxicity_sample\",\n",
    "    dataset_uri=dataset_uri_toxicity.as_posix(),\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation sets\n",
    "We can now create an evaluation set for each algorithm we want to use in our test.\n",
    "\n",
    "More information about metrics and evaluation sets can be found in the paper [\"Evaluating Large Language Models with <i>fmeval</i>\"](https://arxiv.org/pdf/2407.12872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "For the `Summarization` evaluation set, let's replace with your own prompt according to the input signature identified above. `FMeval` uses `$model_input` as placeholder to get the input from your evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = \"Summarize the following text in one sentence: $model_input\"\n",
    "\n",
    "summarization_accuracy = SummarizationAccuracy()\n",
    "\n",
    "evaluation_set_summarization = EvaluationSet(\n",
    "    data_config_summarization,\n",
    "    summarization_accuracy,\n",
    "    summarization_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity\n",
    "\n",
    "Lets do the same for Toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_prompt = \"Complete the following sentence: $model_input\"\n",
    "\n",
    "toxicity = Toxicity(ToxicityConfig(\"detoxify\"))\n",
    "\n",
    "evaluation_set_toxicity = EvaluationSet(\n",
    "    data_config_toxicity,\n",
    "    toxicity,\n",
    "    toxicity_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factual Knowledge\n",
    "\n",
    "And again for Factual Knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual_knowledge_prompt = \"$model_input\"\n",
    "\n",
    "factual_knowledge = FactualKnowledge(\n",
    "    FactualKnowledgeConfig(target_output_delimiter=\"<OR>\")\n",
    ")\n",
    "\n",
    "evaluation_set_factual = EvaluationSet(\n",
    "    data_config_factual_knowledge,\n",
    "    factual_knowledge,\n",
    "    factual_knowledge_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group all evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_list = [\n",
    "    evaluation_set_summarization,\n",
    "    evaluation_set_factual,\n",
    "    evaluation_set_toxicity,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation\n",
    "\n",
    "We setup the MLflow experiment used to track the evaluations.\n",
    "We will then create a new run for each model, and run all the evaluation for that model within that run, so that the metrics will all appear together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `model_id` as run name to make it easier to identify this run as part of the larger experiment, and run the evaluation using the `run_evaluation_sets()` defined in [utils.py](utils.py#20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-simple-runs\"\n",
    "experiment = mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_evaluation_sets(model_runner, evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested runs\n",
    "An alternative approach to organize the runs is to create nested runs for the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fmeval-mlflow-nested-runs\"\n",
    "experiment = mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name, nested=True) as run:\n",
    "    run_evaluation_sets_nested(model_runner, evaluation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation is completed, and the results are recorded in the MLflow tracking server.\n",
    "\n",
    "To continue with the evaluation, you can move to the [compare_models.ipynb](./compare_models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Since SageMaker endpoints are [priced](https://aws.amazon.com/sagemaker/pricing/) by deployed infrastructure time rather than by requests, you can avoid unnecessary charges by deleting your endpoints when you're done experimenting.\n",
    "\n",
    "[Here](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html) you can find instructions on how to delete a SageMaker endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
